{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dc0801",
   "metadata": {},
   "source": [
    "# Gallery Filter Network Demo\n",
    "This notebook implements a demo showcasing the SeqNeXt person search model and Gallery Filter Network (GFN) scoring process.\n",
    "\n",
    "The notebook loads images from the web, and you can easily try it out on other image URLs.\n",
    "\n",
    "All dependencies are imported below, and the model is loaded via torchscript: our package is not used to make the demo (somewhat) self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ddab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch libs\n",
    "import torch\n",
    "## Disable nvfuser for now\n",
    "torch._C._jit_override_can_fuse_on_cpu(False)\n",
    "torch._C._jit_override_can_fuse_on_gpu(False)\n",
    "torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "torch._C._jit_set_nvfuser_enabled(False)\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Libs for data pre-processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations.augmentations.geometric import functional as FGeometric\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Libs for loading images\n",
    "import os\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import ssl\n",
    "## Avoid SSL error\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libs for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ea123",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f418e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from the web\n",
    "def load_web_image(\n",
    "        demo_image_dir='./demo_images',\n",
    "        image_url='https://pics.filmaffinity.com/Friends_TV_Series-783758929-large.jpg',\n",
    "        file_name='demo.png',\n",
    "        display=True,\n",
    "    ):\n",
    "    # Make dir to store the images\n",
    "    if not os.path.exists(demo_image_dir):\n",
    "        os.makedirs(demo_image_dir)\n",
    "\n",
    "    # Download the image URL from the web\n",
    "    urllib.request.urlretrieve(\n",
    "        image_url,\n",
    "        f'{demo_image_dir}/{file_name}')\n",
    "\n",
    "    # Load the image from disk\n",
    "    img = Image.open(f'{demo_image_dir}/{file_name}').convert('RGB')\n",
    "    \n",
    "    # Plot the image and show dimensions\n",
    "    if display:\n",
    "        fix, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b6a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL image to torch tensor\n",
    "def to_tensor(image):\n",
    "    arr = np.array(image)\n",
    "    arr_wrs = window_resize(arr)\n",
    "    tsr = torch.FloatTensor(arr_wrs)\n",
    "    tsr_norm = normalize(tsr)\n",
    "    tsr_input = tsr_norm.permute(2, 0, 1).to(device)\n",
    "    return tsr_input\n",
    "\n",
    "# Convert torch tensor to PIL image\n",
    "def to_image(tensor):\n",
    "    tsr_denorm = denormalize(tensor.permute(1, 2, 0).cpu()).clip(min=0, max=1)\n",
    "    arr = tsr_denorm.numpy()\n",
    "    arr_uint8 = (arr * 255.0).astype(np.uint8)\n",
    "    image = Image.fromarray(arr_uint8)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90c6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image tensor using ImageNet stats\n",
    "def normalize(tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.FloatTensor(mean).view(1, 1, 3)\n",
    "    std = torch.FloatTensor(std).view(1, 1, 3)\n",
    "    return tensor.div(255.0).sub(mean).div(std)\n",
    "\n",
    "# Denormalize image tensor using ImageNet stats\n",
    "def denormalize(tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.FloatTensor(mean).view(1, 1, 3)\n",
    "    std = torch.FloatTensor(std).view(1, 1, 3)\n",
    "    return tensor.mul(std).add(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34671fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image (numpy array) to fit in fixed size window\n",
    "def window_resize(img, min_size=900, max_size=1500, interpolation=cv2.INTER_LINEAR):\n",
    "    height, width = img.shape[:2]\n",
    "    image_min_size = min(width, height)\n",
    "    image_max_size = max(width, height)\n",
    "    scale_factor = min_size / image_min_size\n",
    "    if image_max_size * scale_factor > max_size:\n",
    "        return FGeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n",
    "    else:\n",
    "        return FGeometric.smallest_max_size(img, max_size=min_size, interpolation=interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a8b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot detected boxes on image with matplotlib\n",
    "def show_detects(image, detect, person_sim=None, show_detect_score=False, ax=None, title=None, xlabel=None):\n",
    "    # Setup subplot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    # Setup labels\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=20, fontweight='bold')\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel, fontsize=20)\n",
    "    # Show the image\n",
    "    ax.imshow(denormalize(image.permute(1, 2, 0).cpu()))\n",
    "    # Plot boxes (and optionally similarity scores)\n",
    "    for i, (box, score) in enumerate(zip(detect['det_boxes'].cpu().tolist(), detect['det_scores'].cpu().tolist())):\n",
    "        x, y, x2, y2 = box\n",
    "        w, h = x2 - x, y2 - y\n",
    "        ax.add_patch(Rectangle((x, y), w, h, edgecolor='green', lw=4, fill=False, alpha=0.8))\n",
    "        ax.add_patch(Rectangle((x+2, y+2), w-4, h-4, edgecolor='whitesmoke', lw=1, fill=False, alpha=0.8))\n",
    "        ## Display person similarity if this is supplied\n",
    "        if person_sim is not None:\n",
    "            ax.text(x, y, '{:.2f}'.format(person_sim[i].item()), ha=\"left\", va=\"bottom\", size=14,\n",
    "                bbox=dict(boxstyle=\"square,pad=0.2\", fc=\"whitesmoke\", alpha=0.8, ec='black', lw=2.0)\n",
    "            )\n",
    "        ## Otherwise, display detected box scores\n",
    "        elif show_detect_score:\n",
    "            ax.text(x, y, '{:.2f}'.format(detect['det_scores'][i].item()), ha=\"left\", va=\"bottom\", size=14,\n",
    "                bbox=dict(boxstyle=\"square,pad=0.2\", fc=\"lightblue\", alpha=0.8, ec='black', lw=2.0)\n",
    "            ) \n",
    "    # Remove ticks and expand borders\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    [x.set_linewidth(3) for x in ax.spines.values()]\n",
    "\n",
    "# Return list of detected (PIL) image crops\n",
    "def get_crops(tensor, detect, ax=None):\n",
    "    # Convert tensor back to image\n",
    "    image = to_image(tensor)\n",
    "    # Extract crops using detected boxes\n",
    "    crop_list = []\n",
    "    for i, box in enumerate(detect['det_boxes'].cpu().tolist()):\n",
    "        x1, y1, x2, y2 = box\n",
    "        crop = image.crop((x1, y1, x2, y2))\n",
    "        crop_list.append(crop)\n",
    "    return crop_list\n",
    "\n",
    "# Show ranked re-id crops\n",
    "def show_reid(crop_list, score_list, plot_width=0.75):\n",
    "    # Sort crops by decreasing score\n",
    "    sorted_score_idx = np.argsort(score_list)[::-1]\n",
    "    sorted_score_list = [score_list[i] for i in sorted_score_idx]\n",
    "    sorted_crop_list = [crop_list[1:][i] for i in sorted_score_idx]\n",
    "    # Plotting helper function\n",
    "    def _plot_subplot(_ax, title='', fw=None):\n",
    "        _ax.set_title(title, fontweight=fw, fontsize=11)\n",
    "        _ax.set_xticks([])\n",
    "        _ax.set_yticks([])\n",
    "        [x.set_linewidth(2) for x in _ax.spines.values()]\n",
    "     # Plot query crop\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(crop_list), figsize=(plot_width*len(crop_list), plot_width*3))\n",
    "    ax[0].imshow(crop_list[0].resize((100, 300)))\n",
    "    _plot_subplot(ax[0], title='Query', fw='bold')\n",
    "    # Plot gallery crops\n",
    "    for i, (crop, score) in enumerate(zip(sorted_crop_list, sorted_score_list), 1):\n",
    "        ax[i].imshow(crop.resize((100, 300)))\n",
    "        _plot_subplot(ax[i], title='s={:.2f}'.format(score))\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "        [x.set_linewidth(2) for x in ax[i].spines.values()]\n",
    "    # Return fig\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5fb887",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7b6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "torchscript_path = '../torchscript/cuhk_final_convnext-base_e30.torchscript.pt'\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e61fe4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Query data\n",
    "query_list = [\n",
    "    {\n",
    "        # Chandler \n",
    "        'url': 'https://i.redd.it/uqrvlpf667j51.jpg',\n",
    "        'file': 'chandler_query.jpg',\n",
    "    },\n",
    "    {\n",
    "        # Ross \n",
    "        'url': 'https://metro.co.uk/wp-content/uploads/2019/04/dr3-f247.jpg?quality=90&strip=all&zoom=1&resize=644%2C367',\n",
    "        'file': 'ross_query.jpg',\n",
    "    }\n",
    "]\n",
    "\n",
    "# Gallery data\n",
    "gallery_list = [\n",
    "    {\n",
    "        'url': 'https://deadline.com/wp-content/uploads/2022/10/Screenshot-2022-10-22-at-09.45.12.png?w=681&h=383&crop=1',\n",
    "        'file': 'friends_gallery3.jpg',\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://static.wikia.nocookie.net/friends/images/0/03/TOWChandler%27sWorkLaugh.png/revision/latest/scale-to-width-down/1000?cb=20180307145208',\n",
    "        'file': 'friends_gallery4.jpg',\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb9cab",
   "metadata": {},
   "source": [
    "## Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6d89b2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading query images:\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 101] Network is unreachable>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1350\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 948\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    949\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    715\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 101] Network is unreachable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8920/1528200830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquery_image_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mquery_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_web_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m## Take center crop of query image to make display look better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mquery_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8920/3352951259.py\u001b[0m in \u001b[0;36mload_web_image\u001b[0;34m(demo_image_dir, image_url, file_name, display)\u001b[0m\n\u001b[1;32m     13\u001b[0m     urllib.request.urlretrieve(\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         f'{demo_image_dir}/{file_name}')\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Load the image from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 543\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1393\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/osr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1351\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 101] Network is unreachable>"
     ]
    }
   ],
   "source": [
    "# Load queries\n",
    "print('Loading query images:')\n",
    "query_image_list = []\n",
    "for query_dict in query_list:\n",
    "    query_image = load_web_image(image_url=query_dict['url'], file_name=query_dict['file'])\n",
    "    ## Take center crop of query image to make display look better\n",
    "    query_image = TF.center_crop(query_image, min(query_image.size))\n",
    "    query_image_list.append(query_image)\n",
    "\n",
    "# Load gallery\n",
    "print('Loading gallery images:')\n",
    "gallery_image_list = []\n",
    "for gallery_dict in gallery_list:\n",
    "    gallery_image = load_web_image(image_url=gallery_dict['url'], file_name=gallery_dict['file'])\n",
    "    gallery_image_list.append(gallery_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86104901",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Loading the torchscript model is just a single function call: no libs or configs needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce914d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load torchscript version of model\n",
    "model = torch.jit.load(torchscript_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6647cbf",
   "metadata": {},
   "source": [
    "## Run Model on Query Scenes: Get Query Person and Scene Embeddings\n",
    "\n",
    "There are two ways get query embeddings:\n",
    "1. Let the model try to detect the query bounding box, and use the detected embedding.\n",
    "2. Input the query bounding box explicitly. If the query scene just has one person, you can also supply the full scene extent as the bounding box.\n",
    "\n",
    "Both methods 1. and 2. are done below as an example, but only the detected embedding is used later in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d147949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put query sample through model\n",
    "query_output_list = []\n",
    "with torch.no_grad():\n",
    "    query_tensor_list = [to_tensor(query_image) for query_image in query_image_list]\n",
    "    for query_tensor in query_tensor_list:\n",
    "        ## Use the full scene extent as a bounding box\n",
    "        query_box = torch.FloatTensor([0, 0, *query_tensor.shape[1:]]).unsqueeze(0).to(device)\n",
    "        query_targets = [{'boxes': query_box}]\n",
    "        ## Run query scene through the model\n",
    "        detections = model([query_tensor], query_targets, inference_mode='both')\n",
    "        ## Reorganize results\n",
    "        for query_image, query_detect in zip([query_tensor], detections):\n",
    "            ## Show detected boxes with their scores\n",
    "            show_detects(query_tensor, query_detect, show_detect_score=True)        \n",
    "            query_output_list.append(query_detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7988ac",
   "metadata": {},
   "source": [
    "## Run Model on Gallery Scenes: Get Gallery Person and Scene Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7c274",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Put gallery samples through model\n",
    "gallery_output_list = []\n",
    "with torch.no_grad():\n",
    "    gallery_tensor_list = [to_tensor(image) for image in gallery_image_list]\n",
    "    ## Run query scene through the model\n",
    "    detections = model(gallery_tensor_list, inference_mode='det')\n",
    "    ## Reorganize results\n",
    "    for tensor, detect in zip(gallery_tensor_list, detections):\n",
    "        ## Show detected boxes with their scores\n",
    "        show_detects(tensor, detect, show_detect_score=True)\n",
    "        gallery_output_list.append(detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540ec23",
   "metadata": {},
   "source": [
    "## Get Re-ID Scores: Compare Person Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query image\n",
    "for query_detect in query_output_list:\n",
    "    # Get query person embeddings\n",
    "    query_person_emb = query_detect['det_emb']\n",
    "\n",
    "    # For each gallery image\n",
    "    for gallery_output_dict in gallery_output_list:\n",
    "        ## Get gallery person embeddings\n",
    "        gallery_person_emb = gallery_output_dict['det_emb']\n",
    "\n",
    "        ## Compute person similarity: cosine similarity of person embeddings\n",
    "        person_sim = torch.mm(\n",
    "            F.normalize(query_person_emb, dim=1),\n",
    "            F.normalize(gallery_person_emb, dim=1).T\n",
    "        ).flatten()\n",
    "        \n",
    "        ## Store person similarity\n",
    "        if 'person_sim' not in gallery_output_dict:\n",
    "            gallery_output_dict['person_sim'] = []\n",
    "        gallery_output_dict['person_sim'].append(person_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759f678",
   "metadata": {},
   "source": [
    "## Get GFN Scores: Compare Query-Gated Scene Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query image\n",
    "for query_detect in query_output_list:\n",
    "    # Get query person and scene embeddings\n",
    "    query_person_emb = query_detect['det_emb']\n",
    "    query_scene_emb = query_detect['scene_emb']\n",
    "\n",
    "    # For each gallery image\n",
    "    for gallery_output_dict in gallery_output_list:\n",
    "        ## Get gallery scene embeddings\n",
    "        gallery_scene_emb = gallery_output_dict['scene_emb']\n",
    "\n",
    "        ## Compute query-scene similarity: cosine similarity of query-gated scene embeddings\n",
    "        with torch.no_grad():\n",
    "            qg_scene_sim = model.gfn.get_scores(query_person_emb, query_scene_emb, gallery_scene_emb).flatten().item()\n",
    "        \n",
    "        ## Store query-scene similarity\n",
    "        if 'gfn_sim' not in gallery_output_dict:\n",
    "            gallery_output_dict['gfn_sim'] = []\n",
    "        gallery_output_dict['gfn_sim'].append(qg_scene_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063faf7",
   "metadata": {},
   "source": [
    "## Display Search and Re-ID Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fffc57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute subplot width ratios\n",
    "width_list = [query_tensor_list[0].shape[2]] + [g.shape[2] for g in gallery_tensor_list]\n",
    "width_ratios = [w/sum(width_list) for w in width_list]\n",
    "kw = dict(width_ratios=width_ratios)\n",
    "\n",
    "# For each query image\n",
    "for query_idx, (query_tensor, query_detect) in enumerate(zip(query_tensor_list, query_output_list)):\n",
    "    # Initialize subplots\n",
    "    detect_fig, ax = plt.subplots(nrows=1, ncols=len(width_list), figsize=(20, 12), gridspec_kw=kw)\n",
    "  \n",
    "    # Show query detects\n",
    "    show_detects(query_tensor, query_detect, ax=ax[0], title='Query')\n",
    "\n",
    "    # Get query embeddings\n",
    "    query_crop_list = get_crops(query_tensor, query_detect)\n",
    "\n",
    "    # For each gallery image\n",
    "    full_crop_list, full_score_list = query_crop_list, []\n",
    "    for i, gallery_output_dict in enumerate(gallery_output_list):\n",
    "        ## Get person sim\n",
    "        person_sim = gallery_output_dict['person_sim'][query_idx]\n",
    "        \n",
    "        ## Get query-scene sim\n",
    "        qg_scene_sim = gallery_output_dict['gfn_sim'][query_idx]\n",
    "        \n",
    "        ## Show gallery detects\n",
    "        show_detects(gallery_tensor_list[i], gallery_output_dict, ax=ax[i+1], person_sim=person_sim,\n",
    "            title='Gallery {}'.format(i+1), xlabel='GFN Score: {:.2f}'.format(qg_scene_sim))\n",
    "        \n",
    "        ## Get image crops\n",
    "        crop_list = get_crops(gallery_tensor_list[i], gallery_output_dict)\n",
    "        score_list = person_sim.tolist()\n",
    "        full_crop_list.extend(crop_list)\n",
    "        full_score_list.extend(score_list)\n",
    "        \n",
    "    # Save detect fig\n",
    "    detect_fig.tight_layout()\n",
    "    detect_fig.savefig(f'query_detect{query_idx}.png', bbox_inches='tight', dpi=100)\n",
    "        \n",
    "    # Show ranked re-id crops\n",
    "    reid_fig = show_reid(full_crop_list, full_score_list)\n",
    "    \n",
    "    # Save re-id fig\n",
    "    reid_fig.tight_layout()\n",
    "    reid_fig.subplots_adjust(wspace=0.0)\n",
    "    reid_fig.savefig(f'query_reid{query_idx}.png', bbox_inches='tight', dpi=100)\n",
    "        \n",
    "# Adjust subplot spacing and show plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
